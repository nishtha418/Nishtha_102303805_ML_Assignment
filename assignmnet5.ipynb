{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a618f485-e564-4186-86e3-d76abba1d1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Correlation Matrix:\n",
      "           Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
      "Feature_1   1.000000   0.897925   0.899279   0.895184   0.894056   0.894905   \n",
      "Feature_2   0.897925   1.000000   0.904170   0.914528   0.902848   0.916318   \n",
      "Feature_3   0.899279   0.904170   1.000000   0.906929   0.890779   0.892586   \n",
      "Feature_4   0.895184   0.914528   0.906929   1.000000   0.902122   0.895103   \n",
      "Feature_5   0.894056   0.902848   0.890779   0.902122   1.000000   0.889743   \n",
      "Feature_6   0.894905   0.916318   0.892586   0.895103   0.889743   1.000000   \n",
      "Feature_7   0.903157   0.907611   0.905746   0.902690   0.903508   0.895431   \n",
      "Target      0.903654   0.807414   0.869086   0.854551   0.850811   0.871616   \n",
      "\n",
      "           Feature_7    Target  \n",
      "Feature_1   0.903157  0.903654  \n",
      "Feature_2   0.907611  0.807414  \n",
      "Feature_3   0.905746  0.869086  \n",
      "Feature_4   0.902690  0.854551  \n",
      "Feature_5   0.903508  0.850811  \n",
      "Feature_6   0.895431  0.871616  \n",
      "Feature_7   1.000000  0.829680  \n",
      "Target      0.829680  1.000000  \n",
      "\n",
      "Dataset saved as 'highly_correlated_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Step 2: Define parameters\n",
    "n_samples = 500      # number of data points\n",
    "n_features = 7       # number of features\n",
    "\n",
    "# Step 3: Create a covariance matrix to make features correlated\n",
    "base_corr = 0.9\n",
    "cov = np.full((n_features, n_features), base_corr)\n",
    "np.fill_diagonal(cov, 1.0)  # diagonal = 1 (self-correlation)\n",
    "\n",
    "# Step 4: Generate multivariate normal data (highly correlated)\n",
    "mean = np.zeros(n_features)\n",
    "X = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "\n",
    "# Step 5: Define true coefficients and bias\n",
    "true_weights = np.array([2.5, -1.8, 1.2, 0.8, 0.5, 1.5, -0.7])\n",
    "bias = 3.0\n",
    "\n",
    "# Step 6: Generate target variable with some noise They define how the target variable (y) \n",
    "#is generated from your features (X) using a linear relationship + randomness\n",
    "noise = np.random.normal(0, 1.5, size=n_samples)\n",
    "y = X.dot(true_weights) + bias + noise\n",
    "\n",
    "# Step 7: Create DataFrame\n",
    "columns = [f'Feature_{i+1}' for i in range(n_features)]\n",
    "df = pd.DataFrame(X, columns=columns)\n",
    "df['Target'] = y\n",
    "\n",
    "# Step 8: Display correlation matrix\n",
    "print(\"Feature Correlation Matrix:\")\n",
    "print(df.corr())\n",
    "\n",
    "# Step 9: Save dataset\n",
    "df.to_csv(\"highly_correlated_dataset.csv\", index=False)\n",
    "print(\"\\nDataset saved as 'highly_correlated_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9bea02-a8e3-4a7e-9fd7-9ccb20c6d058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ridge Regression Optimization Complete!\n",
      "\n",
      "Best Parameters:\n",
      "Learning Rate    0.100000\n",
      "Lambda           1.000000\n",
      "Cost             1.078937\n",
      "R2 Score         0.880185\n",
      "Name: 29, dtype: float64\n",
      "\n",
      "Top 5 results by R2 Score:\n",
      "    Learning Rate        Lambda      Cost  R2 Score\n",
      "29            0.1  1.000000e+00  1.078937  0.880185\n",
      "27            0.1  1.000000e-03  1.003463  0.879878\n",
      "26            0.1  1.000000e-05  1.003385  0.879878\n",
      "25            0.1  1.000000e-10  1.003385  0.879878\n",
      "24            0.1  1.000000e-15  1.003385  0.879878\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Load the dataset\n",
    "# ----------------------------\n",
    "df = pd.read_csv(\"highly_correlated_dataset.csv\")\n",
    "X = df.drop(\"Target\", axis=1).values\n",
    "y = df[\"Target\"].values.reshape(-1, 1)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Normalize features\n",
    "# ----------------------------\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Add bias column\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Ridge Regression functions\n",
    "# ----------------------------\n",
    "def ridge_cost(X, y, weights, lmbd):\n",
    "    n = len(y)\n",
    "    predictions = X.dot(weights)\n",
    "    error = predictions - y\n",
    "    mse = (1 / (2 * n)) * np.sum(error ** 2)\n",
    "    reg = (lmbd / (2 * n)) * np.sum(weights[1:] ** 2)\n",
    "    return mse + reg\n",
    "\n",
    "def ridge_gradient(X, y, weights, lmbd):\n",
    "    n = len(y)\n",
    "    predictions = X.dot(weights)\n",
    "    error = predictions - y\n",
    "    grad = (1 / n) * (X.T.dot(error))\n",
    "    grad[1:] += (lmbd / n) * weights[1:]\n",
    "    return np.clip(grad, -1e3, 1e3)  # <-- prevent gradient explosion\n",
    "\n",
    "def gradient_descent(X, y, lr, lmbd, epochs=1000):\n",
    "    weights = np.zeros((X.shape[1], 1))\n",
    "    for _ in range(epochs):\n",
    "        grad = ridge_gradient(X, y, weights, lmbd)\n",
    "        weights -= lr * grad\n",
    "        if np.any(np.isnan(weights)):  # safety stop\n",
    "            print(f\"âš ï¸ NaN detected at lr={lr}, lambda={lmbd}\")\n",
    "            break\n",
    "    return weights\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Try different parameters\n",
    "# ----------------------------\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]   # removed 1 and 10\n",
    "lambdas = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for lmbd in lambdas:\n",
    "        weights = gradient_descent(X_train, y_train, lr, lmbd, epochs=1000)\n",
    "        y_pred = X_test.dot(weights)\n",
    "\n",
    "        if np.any(np.isnan(y_pred)):\n",
    "            continue  # skip invalid runs\n",
    "\n",
    "        cost = ridge_cost(X_test, y_test, weights, lmbd)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        results.append({\n",
    "            \"Learning Rate\": lr,\n",
    "            \"Lambda\": lmbd,\n",
    "            \"Cost\": cost,\n",
    "            \"R2 Score\": r2\n",
    "        })\n",
    "\n",
    "# ----------------------------\n",
    "# Step 5: Display best result\n",
    "# ----------------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "best_row = results_df.loc[results_df[\"R2 Score\"].idxmax()]\n",
    "\n",
    "print(\"âœ… Ridge Regression Optimization Complete!\\n\")\n",
    "print(\"Best Parameters:\")\n",
    "print(best_row)\n",
    "print(\"\\nTop 5 results by R2 Score:\")\n",
    "print(results_df.sort_values(by=\"R2 Score\", ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11fdc867-7f3d-4ec9-b182-d62bf9c89dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset Loaded Successfully!\n",
      "\n",
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
      "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
      "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
      "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
      "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
      "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
      "\n",
      "        B  LSTAT  MEDV  \n",
      "0  396.90   4.98  24.0  \n",
      "1  396.90   9.14  21.6  \n",
      "2  392.83   4.03  34.7  \n",
      "3  394.63   2.94  33.4  \n",
      "4  396.90   5.33  36.2  \n",
      "\n",
      "ðŸ“Š Linear Regression\n",
      "Mean Squared Error: 24.2911\n",
      "RÂ² Score: 0.6688\n",
      "\n",
      "ðŸ“Š Ridge Regression\n",
      "Mean Squared Error: 24.3036\n",
      "RÂ² Score: 0.6686\n",
      "\n",
      "ðŸ“Š Lasso Regression\n",
      "Mean Squared Error: 27.3059\n",
      "RÂ² Score: 0.6276\n",
      "\n",
      "ðŸ” RidgeCV Best Alpha: 7.543120063354623\n",
      "ðŸ” LassoCV Best Alpha: 0.001\n",
      "\n",
      "âœ… RidgeCV RÂ²: 0.6666002024866851\n",
      "âœ… LassoCV RÂ²: 0.6687128230636585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ques 3:\n",
    "# Q3 - Linear, Ridge and Lasso Regression + Cross Validation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 1: Load the Boston Housing Dataset\n",
    "# -----------------------------------------------\n",
    "df = pd.read_csv(\"Boston_Housing.csv\")  \n",
    "\n",
    "print(\"âœ… Dataset Loaded Successfully!\\n\")\n",
    "print(df.head())\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 2: Split Input and Target\n",
    "# -----------------------------------------------\n",
    "X = df.drop(columns=[\"MEDV\"], errors='ignore')  # 'MEDV' is usually the target column\n",
    "y = df[\"MEDV\"]\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 3: Split into Train and Test sets\n",
    "# -----------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 4: Feature Scaling (very important for Ridge/Lasso)\n",
    "# -----------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 5: Train Models\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lin = lin_reg.predict(X_test_scaled)\n",
    "\n",
    "# Ridge Regression (Î» = 0.5748)\n",
    "ridge_reg = Ridge(alpha=0.5748)\n",
    "ridge_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = ridge_reg.predict(X_test_scaled)\n",
    "\n",
    "# Lasso Regression (Î» = 0.5748)\n",
    "lasso_reg = Lasso(alpha=0.5748)\n",
    "lasso_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lasso = lasso_reg.predict(X_test_scaled)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 6: Evaluate Each Model\n",
    "# -----------------------------------------------\n",
    "models = {\n",
    "    \"Linear Regression\": (y_pred_lin, lin_reg),\n",
    "    \"Ridge Regression\": (y_pred_ridge, ridge_reg),\n",
    "    \"Lasso Regression\": (y_pred_lasso, lasso_reg)\n",
    "}\n",
    "\n",
    "for name, (pred, model) in models.items():\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    print(f\"\\nðŸ“Š {name}\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"RÂ² Score: {r2:.4f}\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 7: RidgeCV and LassoCV (Cross Validation)\n",
    "# -----------------------------------------------\n",
    "alphas = np.logspace(-3, 2, 50)  # try 50 values between 10^-3 and 10^2\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nðŸ” RidgeCV Best Alpha:\", ridge_cv.alpha_)\n",
    "print(\"ðŸ” LassoCV Best Alpha:\", lasso_cv.alpha_)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 8: Evaluate CV Models\n",
    "# -----------------------------------------------\n",
    "ridge_cv_pred = ridge_cv.predict(X_test_scaled)\n",
    "lasso_cv_pred = lasso_cv.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nâœ… RidgeCV RÂ²:\", r2_score(y_test, ridge_cv_pred))\n",
    "print(\"âœ… LassoCV RÂ²:\", r2_score(y_test, lasso_cv_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f43da4-0b6c-4715-9e63-826edbeb1a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset Loaded Successfully!\n",
      "\n",
      "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
      "0    293    66      1    30   29     14      1     293     66       1     30   \n",
      "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
      "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
      "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
      "4    321    87     10    39   42     30      2     396    101      12     48   \n",
      "\n",
      "   CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague  \n",
      "0    29      14      A        E      446       33      20     NaN         A  \n",
      "1   414     375      N        W      632       43      10   475.0         N  \n",
      "2   266     263      A        W      880       82      14   480.0         A  \n",
      "3   838     354      N        E      200       11       3   500.0         N  \n",
      "4    46      33      N        E      805       40       4    91.5         N  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 322 entries, 0 to 321\n",
      "Data columns (total 20 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   AtBat      322 non-null    int64  \n",
      " 1   Hits       322 non-null    int64  \n",
      " 2   HmRun      322 non-null    int64  \n",
      " 3   Runs       322 non-null    int64  \n",
      " 4   RBI        322 non-null    int64  \n",
      " 5   Walks      322 non-null    int64  \n",
      " 6   Years      322 non-null    int64  \n",
      " 7   CAtBat     322 non-null    int64  \n",
      " 8   CHits      322 non-null    int64  \n",
      " 9   CHmRun     322 non-null    int64  \n",
      " 10  CRuns      322 non-null    int64  \n",
      " 11  CRBI       322 non-null    int64  \n",
      " 12  CWalks     322 non-null    int64  \n",
      " 13  League     322 non-null    object \n",
      " 14  Division   322 non-null    object \n",
      " 15  PutOuts    322 non-null    int64  \n",
      " 16  Assists    322 non-null    int64  \n",
      " 17  Errors     322 non-null    int64  \n",
      " 18  Salary     263 non-null    float64\n",
      " 19  NewLeague  322 non-null    object \n",
      "dtypes: float64(1), int64(16), object(3)\n",
      "memory usage: 50.4+ KB\n",
      "None\n",
      "\n",
      "ðŸ”¹ Number of missing values before cleaning:\n",
      " AtBat         0\n",
      "Hits          0\n",
      "HmRun         0\n",
      "Runs          0\n",
      "RBI           0\n",
      "Walks         0\n",
      "Years         0\n",
      "CAtBat        0\n",
      "CHits         0\n",
      "CHmRun        0\n",
      "CRuns         0\n",
      "CRBI          0\n",
      "CWalks        0\n",
      "League        0\n",
      "Division      0\n",
      "PutOuts       0\n",
      "Assists       0\n",
      "Errors        0\n",
      "Salary       59\n",
      "NewLeague     0\n",
      "dtype: int64\n",
      "\n",
      "ðŸ”¹ Number of missing values after cleaning:\n",
      " AtBat        0\n",
      "Hits         0\n",
      "HmRun        0\n",
      "Runs         0\n",
      "RBI          0\n",
      "Walks        0\n",
      "Years        0\n",
      "CAtBat       0\n",
      "CHits        0\n",
      "CHmRun       0\n",
      "CRuns        0\n",
      "CRBI         0\n",
      "CWalks       0\n",
      "League       0\n",
      "Division     0\n",
      "PutOuts      0\n",
      "Assists      0\n",
      "Errors       0\n",
      "Salary       0\n",
      "NewLeague    0\n",
      "dtype: int64\n",
      "\n",
      "Categorical Columns: ['League', 'Division', 'NewLeague']\n",
      "\n",
      "ðŸ“Š Model Evaluation Results:\n",
      "\n",
      "Linear Regression\n",
      "Mean Squared Error: 128284.3455\n",
      "RÂ² Score: 0.2907\n",
      "\n",
      "Ridge Regression\n",
      "Mean Squared Error: 126748.9335\n",
      "RÂ² Score: 0.2992\n",
      "\n",
      "Lasso Regression\n",
      "Mean Squared Error: 128269.3493\n",
      "RÂ² Score: 0.2908\n",
      "\n",
      "âœ… Comparison Complete!\n",
      "Generally, Ridge performs better when features are correlated, while Lasso helps in feature selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.530e+06, tolerance: 4.367e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Q2 - Hitters Dataset Analysis with Ridge & Lasso Regression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 1: Load the Dataset\n",
    "# ----------------------------------------------\n",
    "df = pd.read_csv(\"Hitters.csv\")\n",
    "\n",
    "print(\"âœ… Dataset Loaded Successfully!\\n\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 2: Preprocessing\n",
    "# ----------------------------------------------\n",
    "# (a) Handle missing values\n",
    "print(\"\\nðŸ”¹ Number of missing values before cleaning:\\n\", df.isnull().sum())\n",
    "df = df.dropna()  # Drop rows with null values\n",
    "print(\"\\nðŸ”¹ Number of missing values after cleaning:\\n\", df.isnull().sum())\n",
    "\n",
    "# (b) Encode categorical variables\n",
    "# Identify categorical columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "print(\"\\nCategorical Columns:\", list(cat_cols))\n",
    "\n",
    "# Label Encode each categorical column\n",
    "le = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 3: Split into Input and Output Features\n",
    "# ----------------------------------------------\n",
    "# Target variable: 'Salary'\n",
    "X = df.drop(\"Salary\", axis=1)\n",
    "y = df[\"Salary\"]\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 4: Feature Scaling\n",
    "# ----------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 5: Train-Test Split\n",
    "# ----------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 6: Train Models\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_reg = Ridge(alpha=1.0)\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_reg.predict(X_test)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_reg.predict(X_test)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 7: Evaluate Models\n",
    "# ----------------------------------------------\n",
    "models = {\n",
    "    \"Linear Regression\": y_pred_lin,\n",
    "    \"Ridge Regression\": y_pred_ridge,\n",
    "    \"Lasso Regression\": y_pred_lasso\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“Š Model Evaluation Results:\")\n",
    "for name, preds in models.items():\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"RÂ² Score: {r2:.4f}\")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 8: Compare and Analyze\n",
    "# ----------------------------------------------\n",
    "print(\"\\nâœ… Comparison Complete!\")\n",
    "print(\"Generally, Ridge performs better when features are correlated, while Lasso helps in feature selection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be89e8d7-3eea-441e-b464-8c77fe326bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Accuracy on test data: 90.00%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  8  2]\n",
      " [ 0  1  9]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.89      0.80      0.84        10\n",
      "   virginica       0.82      0.90      0.86        10\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.90        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4 - Multiclass Logistic Regression (One-vs-Rest) from scratch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 1: Load the Iris dataset\n",
    "# ----------------------------------------------------\n",
    "iris = load_iris()\n",
    "X = iris.data        # 4 features\n",
    "y = iris.target      # 3 classes: 0,1,2\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Add bias term\n",
    "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 2: Define sigmoid & gradient descent functions\n",
    "# ----------------------------------------------------\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, weights):\n",
    "    n = len(y)\n",
    "    h = sigmoid(X.dot(weights))\n",
    "    epsilon = 1e-9  # to avoid log(0)\n",
    "    cost = (-1 / n) * (y.T.dot(np.log(h + epsilon)) + (1 - y).T.dot(np.log(1 - h + epsilon)))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, lr=0.1, epochs=1000):\n",
    "    weights = np.zeros((X.shape[1], 1))\n",
    "    for _ in range(epochs):\n",
    "        h = sigmoid(X.dot(weights))\n",
    "        gradient = (1 / len(y)) * X.T.dot(h - y)\n",
    "        weights -= lr * gradient\n",
    "    return weights\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 3: One-vs-Rest Training\n",
    "# ----------------------------------------------------\n",
    "num_classes = len(np.unique(y_train))\n",
    "weights_all = []\n",
    "\n",
    "for c in range(num_classes):\n",
    "    # Create binary labels for class c\n",
    "    y_binary = (y_train == c).astype(int).reshape(-1, 1)\n",
    "    weights_c = gradient_descent(X_train, y_binary, lr=0.1, epochs=2000)\n",
    "    weights_all.append(weights_c)\n",
    "\n",
    "weights_all = np.hstack(weights_all)  # shape: (features, classes)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 4: Prediction\n",
    "# ----------------------------------------------------\n",
    "def predict(X, weights_all):\n",
    "    probs = sigmoid(X.dot(weights_all))  # get probabilities for each class\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    return preds\n",
    "\n",
    "y_pred = predict(X_test, weights_all)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 5: Evaluation\n",
    "# ----------------------------------------------------\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"âœ… Accuracy on test data: {acc*100:.2f}%\\n\")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c6158-17ce-4324-849c-43e33d776adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
