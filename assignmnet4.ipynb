{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3500f716-bb94-41c1-ba5c-249884a7eb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total books scraped: 1000\n",
      "[('A Light in the Attic', 'Â£51.77', 'In stock', 'Three'), ('Tipping the Velvet', 'Â£53.74', 'In stock', 'One'), ('Soumission', 'Â£50.10', 'In stock', 'One'), ('Sharp Objects', 'Â£47.82', 'In stock', 'Four'), ('Sapiens: A Brief History of Humankind', 'Â£54.23', 'In stock', 'Five')]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://books.toscrape.com/\"\n",
    "all_books = []\n",
    "\n",
    "while url:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    books = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    for book in books:\n",
    "        title = book.h3.a[\"title\"]\n",
    "        price = book.find(\"p\", class_=\"price_color\").text\n",
    "        availability = book.find(\"p\", class_=\"instock availability\").text.strip()\n",
    "        rating = book.p[\"class\"][1]\n",
    "        all_books.append((title, price, availability, rating))\n",
    "\n",
    "    # Find next page\n",
    "    next_button = soup.find(\"li\", class_=\"next\")\n",
    "    if next_button:\n",
    "        next_page = next_button.a[\"href\"]\n",
    "        url = requests.compat.urljoin(url, next_page)  # Build full URL for next page\n",
    "    else:\n",
    "        url = None  # Stop when no more pages\n",
    "\n",
    "print(\"Total books scraped:\", len(all_books))\n",
    "print(all_books[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7715e5-064c-4d8e-98e2-7b078662b58d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Start Chrome (Selenium Manager automatically finds driver)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open IMDB Top 250\n",
    "driver.get(\"https://www.imdb.com/chart/top/\")\n",
    "\n",
    "# Wait for page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Find all movie containers\n",
    "movies = driver.find_elements(By.CSS_SELECTOR, \"li.ipc-metadata-list-summary-item\")\n",
    "\n",
    "movie_data = []\n",
    "\n",
    "for movie in movies:\n",
    "    # Rank\n",
    "    rank = movie.find_element(By.CSS_SELECTOR, \"span.ipc-title__text\").text.split('.')[0]\n",
    "    \n",
    "    # Title\n",
    "    title = movie.find_element(By.CSS_SELECTOR, \"h3.ipc-title__text\").text.split('. ')[-1]\n",
    "    \n",
    "    # Year of release\n",
    "    year = movie.find_element(By.CSS_SELECTOR, \"span.sc-b189961a-8\").text\n",
    "    \n",
    "    # IMDB Rating\n",
    "    rating = movie.find_element(By.CSS_SELECTOR, \"span.ipc-rating-star--rating\").text\n",
    "    \n",
    "    movie_data.append({\n",
    "        \"Rank\": int(rank),\n",
    "        \"Movie Title\": title,\n",
    "        \"Year of Release\": year,\n",
    "        \"IMDB Rating\": rating\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(movie_data)\n",
    "\n",
    "# Export to CSV\n",
    "df.to_csv(\"imdb_top250.csv\", index=False)\n",
    "\n",
    "print(\"✅ IMDB Top 250 Movies scraped successfully!\")\n",
    "print(df.head())\n",
    "\n",
    "# Close browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ebe3ba4-142f-44c1-8579-693064169b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Access is denied.\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0ae7b1-e41c-48ca-a084-b6e9f8fc12d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Weather data scraped successfully!\n",
      "     City Name  Temperature Weather Condition\n",
      "0        Accra  शुक्र 03.58                  \n",
      "1  Addis Ababa  शुक्र 06.58                  \n",
      "2   Adelaide *  शुक्र 14.28                  \n",
      "3      Algiers  शुक्र 04.58                  \n",
      "4       Almaty  शुक्र 08.58                  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.timeanddate.com/weather/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Try to find the main table dynamically\n",
    "table = soup.find(\"table\")\n",
    "if not table:\n",
    "    print(\"❌ Could not find the weather table on the page.\")\n",
    "else:\n",
    "    cities_data = []\n",
    "\n",
    "    # Each row represents one city (skip header)\n",
    "    for row in table.find_all(\"tr\")[1:]:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) >= 3:\n",
    "            city_name = cols[0].text.strip()\n",
    "            temperature = cols[1].text.strip()\n",
    "            condition = cols[2].text.strip()\n",
    "            cities_data.append({\n",
    "                \"City Name\": city_name,\n",
    "                \"Temperature\": temperature,\n",
    "                \"Weather Condition\": condition\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(cities_data)\n",
    "\n",
    "    # Export to CSV\n",
    "    df.to_csv(\"weather.csv\", index=False)\n",
    "\n",
    "    print(\"✅ Weather data scraped successfully!\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fb35b8-ef04-4e46-9431-0e89e0e38a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
